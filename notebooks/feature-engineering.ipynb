{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering and Data Preprocessing\n",
        "\n",
        "This notebook performs feature engineering, data transformation, and handles class imbalance for both datasets as part of Task 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "from data_cleaning import clean_fraud_data, clean_creditcard_data, ip_to_integer, merge_ip_to_country\n",
        "from feature_engineering import create_time_features, create_transaction_frequency_features, create_aggregated_features\n",
        "from data_transformation import prepare_data_for_modeling, encode_categorical_features, scale_numerical_features, handle_class_imbalance\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Fraud_Data.csv Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and clean Fraud_Data\n",
        "print(\"=\" * 60)\n",
        "print(\"Processing Fraud_Data.csv\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fraud_df = pd.read_csv('../data/raw/Fraud_Data.csv')\n",
        "print(f\"Original shape: {fraud_df.shape}\")\n",
        "\n",
        "# Clean data\n",
        "fraud_df_clean = clean_fraud_data(fraud_df)\n",
        "print(f\"Shape after cleaning: {fraud_df_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geolocation Integration\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Geolocation Integration\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load IP to Country mapping\n",
        "ip_country_df = pd.read_csv('../data/raw/IpAddress_to_Country.csv')\n",
        "print(f\"IP Country mapping shape: {ip_country_df.shape}\")\n",
        "\n",
        "# Convert IP address columns to integer\n",
        "ip_country_df['lower_bound_ip_address'] = ip_country_df['lower_bound_ip_address'].apply(ip_to_integer)\n",
        "ip_country_df['upper_bound_ip_address'] = ip_country_df['upper_bound_ip_address'].apply(ip_to_integer)\n",
        "\n",
        "# Merge fraud data with country mapping\n",
        "fraud_df_with_country = merge_ip_to_country(fraud_df_clean, ip_country_df)\n",
        "print(f\"Shape after country merge: {fraud_df_with_country.shape}\")\n",
        "print(f\"Rows matched: {len(fraud_df_with_country)} / {len(fraud_df_clean)} ({len(fraud_df_with_country)/len(fraud_df_clean)*100:.2f}%)\")\n",
        "\n",
        "# Analyze fraud patterns by country\n",
        "if 'country' in fraud_df_with_country.columns:\n",
        "    country_fraud = fraud_df_with_country.groupby('country').agg({\n",
        "        'class': ['count', 'sum', 'mean']\n",
        "    }).reset_index()\n",
        "    country_fraud.columns = ['country', 'total_transactions', 'fraud_count', 'fraud_rate']\n",
        "    print(f\"\\nTop 5 countries by fraud rate:\")\n",
        "    print(country_fraud.sort_values('fraud_rate', ascending=False).head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create time-based features\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Creating Time-based Features\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fraud_df_feat = create_time_features(fraud_df_with_country)\n",
        "print(\"Time features created:\")\n",
        "time_features = ['hour_of_day', 'day_of_week', 'time_since_signup']\n",
        "for feat in time_features:\n",
        "    if feat in fraud_df_feat.columns:\n",
        "        print(f\"  - {feat}\")\n",
        "\n",
        "# Display statistics\n",
        "if 'time_since_signup' in fraud_df_feat.columns:\n",
        "    print(f\"\\ntime_since_signup statistics:\")\n",
        "    print(fraud_df_feat['time_since_signup'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create transaction frequency features\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Creating Transaction Frequency Features\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fraud_df_feat = create_transaction_frequency_features(fraud_df_feat, time_windows=[1, 6, 24])\n",
        "print(\"Frequency features created:\")\n",
        "freq_features = [col for col in fraud_df_feat.columns if 'transactions_in' in col or 'transaction_velocity' in col]\n",
        "for feat in freq_features:\n",
        "    print(f\"  - {feat}\")\n",
        "\n",
        "# Display statistics\n",
        "if 'transaction_velocity' in fraud_df_feat.columns:\n",
        "    print(f\"\\ntransaction_velocity statistics:\")\n",
        "    print(fraud_df_feat['transaction_velocity'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create aggregated features\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Creating Aggregated Features\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fraud_df_feat = create_aggregated_features(fraud_df_feat)\n",
        "print(\"Aggregated features created:\")\n",
        "agg_features = [col for col in fraud_df_feat.columns if col.startswith('user_') and col != 'user_id']\n",
        "for feat in agg_features:\n",
        "    print(f\"  - {feat}\")\n",
        "\n",
        "print(f\"\\nFinal feature count: {len(fraud_df_feat.columns)}\")\n",
        "print(f\"Final shape: {fraud_df_feat.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify feature types for transformation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Preparing Data for Transformation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['source', 'browser', 'sex', 'country']\n",
        "categorical_cols = [col for col in categorical_cols if col in fraud_df_feat.columns]\n",
        "\n",
        "# Numerical columns (exclude target and ID columns)\n",
        "numerical_cols = fraud_df_feat.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numerical_cols = [col for col in numerical_cols if col not in ['class', 'user_id', 'device_id', 'ip_integer', \n",
        "                                                                 'lower_bound_ip_address', 'upper_bound_ip_address']]\n",
        "\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols[:10]}... (showing first 10)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Transformation and Class Imbalance Handling\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Data Transformation and Class Imbalance Handling\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Prepare data for modeling\n",
        "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud, transformers_fraud = prepare_data_for_modeling(\n",
        "    fraud_df_feat,\n",
        "    target_col='class',\n",
        "    categorical_cols=categorical_cols,\n",
        "    numerical_cols=numerical_cols,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    handle_imbalance=True,\n",
        "    imbalance_method='smote'  # Using SMOTE as recommended\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train_fraud.shape}\")\n",
        "print(f\"Test set shape: {X_test_fraud.shape}\")\n",
        "print(f\"\\nTraining target distribution:\")\n",
        "print(y_train_fraud.value_counts())\n",
        "print(f\"\\nTest target distribution:\")\n",
        "print(y_test_fraud.value_counts())\n",
        "\n",
        "# Save processed data\n",
        "fraud_df_feat.to_csv('../data/processed/fraud_data_processed.csv', index=False)\n",
        "X_train_fraud.to_csv('../data/processed/fraud_X_train.csv', index=False)\n",
        "X_test_fraud.to_csv('../data/processed/fraud_X_test.csv', index=False)\n",
        "y_train_fraud.to_csv('../data/processed/fraud_y_train.csv', index=False)\n",
        "y_test_fraud.to_csv('../data/processed/fraud_y_test.csv', index=False)\n",
        "\n",
        "print(\"\\nProcessed data saved to data/processed/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: creditcard.csv Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and clean creditcard data\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Processing creditcard.csv\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cc_df = pd.read_csv('../data/raw/creditcard.csv')\n",
        "print(f\"Original shape: {cc_df.shape}\")\n",
        "\n",
        "# Clean data\n",
        "cc_df_clean = clean_creditcard_data(cc_df)\n",
        "print(f\"Shape after cleaning: {cc_df_clean.shape}\")\n",
        "\n",
        "# For creditcard data, features are already PCA-transformed\n",
        "# We just need to scale and handle imbalance\n",
        "print(\"\\nNote: Credit card data features (V1-V28) are already PCA-transformed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare creditcard data for modeling\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Data Transformation and Class Imbalance Handling\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Identify numerical columns (all V1-V28, Time, Amount)\n",
        "numerical_cols_cc = [f'V{i}' for i in range(1, 29)] + ['Time', 'Amount']\n",
        "numerical_cols_cc = [col for col in numerical_cols_cc if col in cc_df_clean.columns]\n",
        "\n",
        "# No categorical columns for creditcard data\n",
        "categorical_cols_cc = []\n",
        "\n",
        "# Prepare data for modeling\n",
        "X_train_cc, X_test_cc, y_train_cc, y_test_cc, transformers_cc = prepare_data_for_modeling(\n",
        "    cc_df_clean,\n",
        "    target_col='Class',\n",
        "    categorical_cols=categorical_cols_cc,\n",
        "    numerical_cols=numerical_cols_cc,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    handle_imbalance=True,\n",
        "    imbalance_method='smote'  # Using SMOTE as recommended\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train_cc.shape}\")\n",
        "print(f\"Test set shape: {X_test_cc.shape}\")\n",
        "print(f\"\\nTraining target distribution:\")\n",
        "print(y_train_cc.value_counts())\n",
        "print(f\"\\nTest target distribution:\")\n",
        "print(y_test_cc.value_counts())\n",
        "\n",
        "# Save processed data\n",
        "cc_df_clean.to_csv('../data/processed/creditcard_processed.csv', index=False)\n",
        "X_train_cc.to_csv('../data/processed/creditcard_X_train.csv', index=False)\n",
        "X_test_cc.to_csv('../data/processed/creditcard_X_test.csv', index=False)\n",
        "y_train_cc.to_csv('../data/processed/creditcard_y_train.csv', index=False)\n",
        "y_test_cc.to_csv('../data/processed/creditcard_y_test.csv', index=False)\n",
        "\n",
        "print(\"\\nProcessed data saved to data/processed/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Task 1 Completion Checklist:\n",
        "\n",
        "✅ **Data Cleaning**\n",
        "- Handled missing values (imputed with median/mode)\n",
        "- Removed duplicates\n",
        "- Corrected data types (timestamps, numerical)\n",
        "\n",
        "✅ **Exploratory Data Analysis**\n",
        "- Univariate analysis: distributions of key variables\n",
        "- Bivariate analysis: relationships between features and target\n",
        "- Class distribution analysis: quantified the imbalance\n",
        "\n",
        "✅ **Geolocation Integration**\n",
        "- Converted IP addresses to integer format\n",
        "- Merged Fraud_Data.csv with IpAddress_to_Country.csv using range-based lookup\n",
        "- Analyzed fraud patterns by country\n",
        "\n",
        "✅ **Feature Engineering (for Fraud_Data.csv)**\n",
        "- Transaction frequency and velocity features\n",
        "- Time-based features: hour_of_day, day_of_week, time_since_signup\n",
        "- Aggregated user-level features\n",
        "\n",
        "✅ **Data Transformation**\n",
        "- Normalized/scaled numerical features (StandardScaler)\n",
        "- Encoded categorical features (One-Hot Encoding)\n",
        "\n",
        "✅ **Class Imbalance Handling**\n",
        "- Applied SMOTE to training data only\n",
        "- Documented class distribution before and after resampling\n",
        "- Justified choice: SMOTE creates synthetic samples of minority class, preserving information while balancing classes\n",
        "\n",
        "### Key Decisions:\n",
        "\n",
        "1. **SMOTE vs Undersampling**: Chose SMOTE because it preserves all data points and creates synthetic samples, which is better for highly imbalanced datasets where we have limited fraud cases.\n",
        "\n",
        "2. **Feature Engineering**: Created time-based and frequency features to capture behavioral patterns that are strong indicators of fraud.\n",
        "\n",
        "3. **Geolocation**: Integrated country information to identify high-risk regions for fraud.\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to Task 2: Model Building and Training\n",
        "- Use the processed datasets saved in `data/processed/`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
